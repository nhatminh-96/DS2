{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Altegrad_2021_Lab2_NMT_Handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2021<br>Lab Session 2: NMT</h2><h3> Neural Machine Translation</h3> 16 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib\n",
        "import numpy as np"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        embedded_input = self.embedding(input)\n",
        "        hs, _ = self.rnn(embedded_input)\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "        return hs"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "    \n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = torch.tanh((self.ff_concat(torch.cat((target_h_rep, source_hs), dim=2))))\n",
        "        scores = self.ff_score(concat_output) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes \n",
        "        return ct, norm_scores"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, input, source_context, h):\n",
        "        embedded_input = self.embedding(input)\n",
        "        hs, h = self.rnn(embedded_input, h)\n",
        "        tilde_h = torch.tanh(self.ff_concat(torch.cat((source_context, h), dim=2)))\n",
        "        prediction = self.predict(tilde_h)\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "\n",
        "        return prediction, h"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "        \n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "        \n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "        \n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "    \n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "    \n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod: \n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "        \n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        scores = []\n",
        "        \n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context, score = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "                scores.append(score.squeeze().cpu().detach().numpy())\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = torch.max(prediction, 2)[1]\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "        \n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "        \n",
        "        return to_return, scores\n",
        "    \n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "        \n",
        "        for epoch in range(n_epochs): \n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)                        \n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "                        \n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "                        \n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()                        \n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)                       \n",
        "                        pbar.set_postfix(tdqm_dict)                     \n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "            \n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "            \n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "    \n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "        \n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints \n",
        "    \n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "    \n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits, _ = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl)\n",
        "        \n",
        "    def predict_get_attention(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits, scores = self.forward(source_ints, self.max_size, True)  # (seq) -> (<=max_size,vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze()  # (<=max_size,1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        \n",
        "        return target_nl, np.array(scores)\n",
        "        '''\n",
        "        font = {'family': 'normal',\n",
        "                'weight': 'bold',\n",
        "                'size': 13}\n",
        "\n",
        "        matplotlib.rc('font', **font)\n",
        "\n",
        "        fig = plt.figure(figsize=(6, 7))\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(np.array(scores[:10]))\n",
        "        fig.colorbar(cax)\n",
        "        source_arr = source_nl.split()\n",
        "        ax.set_xticklabels([''] + [x for x in source_arr])\n",
        "        ax.set_yticklabels([''] + [x for x in target_nl])\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        plt.savefig('attention' + str(idx) + '.pdf')\n",
        "        '''\n",
        "\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "    \n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new        "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "datl5SFtJ9Br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1015be54-4572-4109-99cc-69def3c17033"
      },
      "source": [
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\" -O \"data.zip\"\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n",
        "!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 21:04:58--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.43.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.43.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtdqw.am.files.1drv.com/y4mn9DDSYDvPdRMa0ZqbrTR2Qsu3csLVbHSxuPI8ldv1qecQ3-9_4TwlYANvPMjvaOB5h8MwzdckTw_ONYkfZRoDYbVKstzSGfDtXfCqH2oMj3PIuSuwC63sL7h-FJlJ2ATcy0NXzmsXzs-SoxPILP6YSqwfWWOdAGnht8-ChyYsvTs0YczUoNDxZhW6CvDTCSIqT-KQvIHJ8HOOIzIPeH3UA/data.zip?download&psid=1 [following]\n",
            "--2021-11-23 21:04:59--  https://vgtdqw.am.files.1drv.com/y4mn9DDSYDvPdRMa0ZqbrTR2Qsu3csLVbHSxuPI8ldv1qecQ3-9_4TwlYANvPMjvaOB5h8MwzdckTw_ONYkfZRoDYbVKstzSGfDtXfCqH2oMj3PIuSuwC63sL7h-FJlJ2ATcy0NXzmsXzs-SoxPILP6YSqwfWWOdAGnht8-ChyYsvTs0YczUoNDxZhW6CvDTCSIqT-KQvIHJ8HOOIzIPeH3UA/data.zip?download&psid=1\n",
            "Resolving vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)... 13.107.43.12\n",
            "Connecting to vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)|13.107.43.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2021-11-23 21:04:59--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.43.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.43.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtcqw.am.files.1drv.com/y4mhgbeLJ0G5-unHpj5NXJwgO-4c0zuXssljc6yJ01CqDclM0QCkDOdH74BiXNK1UjdWN7qIAYnYT5zLt-yfYimLhHsUHKtvCA2ONIxyAeDAhrn6C4qLrb46EyxiU5z_SQTnereuUO83MauqGT844-okNdeegX_qp7HLX8ONkBRC620g16F2AuPaDRknV2PpozK85SoFr3WNiev3Nhmy6_94w/pretrained_moodle.pt?download&psid=1 [following]\n",
            "--2021-11-23 21:05:00--  https://vgtcqw.am.files.1drv.com/y4mhgbeLJ0G5-unHpj5NXJwgO-4c0zuXssljc6yJ01CqDclM0QCkDOdH74BiXNK1UjdWN7qIAYnYT5zLt-yfYimLhHsUHKtvCA2ONIxyAeDAhrn6C4qLrb46EyxiU5z_SQTnereuUO83MauqGT844-okNdeegX_qp7HLX8ONkBRC620g16F2AuPaDRknV2PpozK85SoFr3WNiev3Nhmy6_94w/pretrained_moodle.pt?download&psid=1\n",
            "Resolving vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)... 13.107.43.12\n",
            "Connecting to vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)|13.107.43.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Archive:  data.zip\n",
            "replace pairs_test_ints.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ-cvSuLQVt"
      },
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "        \n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "    \n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "    \n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "    \n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "    \n",
        "    print('data loaded')\n",
        "        \n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "    \n",
        "    print('data prepared')\n",
        "    \n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "    \n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "    \n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXbQjP_YrgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc1bc86-54f3-4895-e83e-e78c8953f842"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "    \n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HWiIjjG-OZE"
      },
      "source": [
        "Function to catch attention matrix and plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXBY6Ud2ky8u"
      },
      "source": [
        "def plot_attention(source_nl):\n",
        "    target_nl, attention = model.predict_get_attention(source_nl)\n",
        "    print(\"Source:\" + source_nl)\n",
        "    print(\"Translated: \" + \"\".join(str(item) + \" \" for item in target_nl))\n",
        "    fig = plt.figure(figsize=(10, 15))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.imshow(attention[:15], cmap=\"gray\")\n",
        "    #print(attention.shape)\n",
        "    #fig.colorbar(cax)\n",
        "    source_arr = source_nl.split()\n",
        "    ax.set_xticklabels([''] + [x for x in source_arr])\n",
        "    ax.set_yticklabels([''] + [x for x in target_nl])\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    #plt.savefig('attention' + source_nl + '.pdf')\n"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga24yb5q-Uks"
      },
      "source": [
        "Try with a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "y8bu-CcqmK2Q",
        "outputId": "580e051a-6b61-40e7-fcab-ae5b4b3d32e8"
      },
      "source": [
        "sentence = \"the ocean is full of delicious fish\"\n",
        "plot_attention(sentence)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source:the ocean is full of delicious fish\n",
            "Translated: l océan est pleine de poisson poisson poisson poisson poisson poisson chaud délicieux chaud intéressant chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud chaud \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAANOCAYAAACRIkw7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3cfbSedX3n+/cnhAchEA9grTJqRFoiRR5C1KLUg+CoPU6nuqqLGUGlZzxZKDWtHM/xiVWjc+zpw3EeoFJXmglBYGoHsExHT5UWDCCKJoAEjEBPkdbpuGozmFKlIKHf88e+0tns7nwDJPu+773zfq2VxbV/18P+XtlJ3lz3vZNUFZIkaXaLxj2AJEmTzFBKktQwlJIkNQylJEkNQylJUmPxuAeYVAcddFAdcsgh4x5jrzv00EPHPcKc+Yu/+ItxjzAn/M50aSS2VdWzZtthKHfhkEMO4Wd/9mfHPcZed8YZZ4x7hDnz7ne/e9wjzIlHH3103CNI+4I/39UOX3qVJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlq7JOhTPKDcc8gSZof9slQSpL0ZBlKSZIahlKSpMbicQ8wSZKsAlYBHHzwwWOeRpI0CXyinKaq1lbVyqpaedBBB417HEnSBDCUkiQ1DKUkSY19MpRVtWTcM0iS5od9MpSSJD1ZhlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqpqnHPMJGSLMifmG3bto17hDlz7LHHjnuEOfHggw+Oe4Q5458/miC3VdXK2Xb4RClJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNeZ1KJMsTvJLSQ4c9yySpIVp3oYySYB/B2ypqkfHPY8kaWFaPO4Bnq6qKuCXxj2HJGlhm6gnyiQXJLl7+PErw9rbk2xJcmeSy4e1ZyW5Jsmm4ccrh/WXJflqkjuSfCXJscP6uUk+m+QLSf40yW+O7y4lSfPJxDxRJjkF+EXg5UCAryXZBFwIvKKqtiU5fDj83wP/tqq+nOT5wBeBFwP3AD9TVTuSvAb4NeAXhnNOAk4GHgXuTXJxVX1nxgyrgFVzeqOSpHllYkIJnAb8QVX9ECDJZ4GVwFVVtQ2gqh4cjn0NcNzU25QAHJZkCbAUuCzJTwAF7D/t+tdX1d8M194KvAB4Qiirai2wdjim9vodSpLmnUkK5VOxCPjpqnpk+mKS3wa+VFVvSrIM2Dht9/Rv+Hmc+XvvkqQRmqT3KG8G3pjk4CSHAG8CNgNvSXIEwLSXXq8D3rPzxCQnDZtLgb8cts8dxdCSpIVtYkJZVbcDG4CvA18D1lXVLcDHgRuT3An8m+Hw1cDK4Zt8tgLnDeu/CfzfSe7AJ0ZJ0l6Qqb9loZkW6nuU27ZtG/cIc+bYY48d9whz4sEHH9z9QfOUf/5ogtxWVStn2zExT5SSJE0iQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSY3F4x5Ao/XjP/7j4x5hznz3u98d9whz4oUvfOG4R5gzjzzyyLhHmBM7duwY9wjai3yilCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIa+1wok5yb5LnjnkOSND/sc6EEzgUMpSTpSVk87gH2liTnAKuBA4CvAe8G/gOwEihgPfCd4eMrk/wdcGpV/d14JpYkzQcLIpRJXgycBbyyqh5LcglwIXBUVR0/HPPMqtqe5JeA91XV5lmuswpYNcrZJUmTbaG89HomcAqwKck3ho8PB45OcnGS1wMP7e4iVbW2qlZW1cq5HVeSNF8slFAGuKyqThp+HFtVvwycCGwEzgPWjXNASdL8tFBCeT3w5iQ/BpDk8CQvABZV1TVMvQy7Yjj2b4FDxzOmJGm+WRDvUVbV1iQXAtclWQQ8BlwA/MHwMcAHh/9uAD7lN/NIkp6MVNW4Z5hISRbkT8zixQvi/41m9d3vfnfcI8yJF77wheMeYc488sgj4x5hTuzYsWPcI+ipu21X35+yUF56lSRpThhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqLB73ABqtHTt2jHuEOfPc5z533CPMiauvvnrcI8yZiy++eNwjzInrr79+3CPMif3222/cI8yZ7s9GnyglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpMbEhzLJxiQrd3PMuiTHjWomSdK+Y/G4B9gbquqd455BkrQwTcwTZZJlSe5JcmWSbyW5OsnBM455bZKvJrk9yVVJlgzr//DUmeQHST6e5M4ktyZ59rD+rCTXJNk0/Hjl6O9SkjTfTEwoB8cCl1TVi4GHgHfv3JHkSOBC4DVVtQLYDFwwyzUOAW6tqhOBm4D/bVj/98C/raqXAr8ArJuzu5AkLRiT9tLrd6rqlmH7CmD1tH0/DRwH3JIE4ADgq7Nc40fA54bt24B/Omy/BjhuOBfgsCRLquoHOxeSrAJW7YX7kCQtEJMWymo+DvDHVfUvd3ONx6pq53mP8z/ucRHw01X1yC4/edVaYC1AkpmzSJL2QZP20uvzk5w6bL8V+PK0fbcCr0xyDECSQ5L85FO49nXAe3Z+kOSkPR1WkrTwTVoo7wXOT/It4H8Cfmfnjqr6a+Bc4PeSbGHqZdflT+Haq4GVSbYk2Qqct9emliQtWJP20uuOqjpnxtrpOzeq6gbgpTNPqqrpxyyZtn01cPWwvQ04a++OK0la6CbtiVKSpIkyMU+UVfUAcPy455AkaTqfKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqTG4nEPIO0tjz322LhHmBMbN24c9whz5uUvf/m4R5gTC/VrtmTJknGPMGe2b9++y30+UUqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVJjQYcyyZok7xv3HJKk+WtBh1KSpD214EKZ5MNJ7kvyZeDYYe1FSb6Q5LYkNydZPuYxJUnzxOJxD7A3JTkF+BfASUzd2+3AbcBa4Lyq+tMkLwcuAc6Y5fxVwKrRTSxJmnQLKpTAzwB/UFUPAyT5Q+Ag4BXAVUl2HnfgbCdX1VqmokqSmvNpJUkTb6GFcjaLgO1VddK4B5EkzT8L7T3Km4A3JnlGkkOBnwMeBr6d5C0AmXLiOIeUJM0fCyqUVXU78PvAncAfAZuGXWcD/yrJncA3gZ8fz4SSpPlmwb30WlUfBz4+y67Xj3oWSdL8t6CeKCVJ2tsMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNVJV455hIiXxJ0aaY0nGPcKcOPnkk8c9wpxYt27duEeYMytWrLitqlbOts8nSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpMbGhTLIuyXHjnkOStG9bPO4BdqWq3jnuGSRJGtkTZZJlSe5JcmWSbyW5OsnBSc5MckeSu5KsT3LgcPzGJCuT7JdkQ5K7h2PeO+xfnWRrki1JPjOsHZ7k2mHt1iQnDOtrhmtvTHJ/ktWjum9J0vw26pdejwUuqaoXAw8BFwAbgLOq6iVMPeG+a8Y5JwFHVdXxwzGXDusfAE6uqhOA84a1jwJ3DGsfAj497TrLgdcBLwM+kmT/mcMlWZVkc5LNe36rkqSFYNSh/E5V3TJsXwGcCXy7qu4b1i4DXjXjnPuBo5NcnOT1TAUWYAtwZZJzgB3D2mnA5QBVdQNwRJLDhn2fr6pHq2ob8D3g2TOHq6q1VbWyqlbu8Z1KkhaEUYeyZny8fbcnVH0fOBHYyNST47ph1xuATwIrgE1Jdvd+66PTth9ngt+flSRNjlGH8vlJTh223wpsBpYlOWZYextw4/QTkhwJLKqqa4ALgRVJFgHPq6ovAe8HlgJLgJuBs4fzTge2VdVDSJL0NI36qepe4Pwk64GtwGrgVuCq4YlwE/CpGeccBVw6xBHgg8B+wBVJlgIBLqqq7UnWAOuTbAEeBt4x1zckSVrYRh3KHVV1zoy164GTZx5YVadP+3DFLNc6bZZzHgTeOMv6mhkfH/8kZpUkaXL/wQFJkibByJ4oq+oBwCc5SdK84hOlJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1Fg87gEk7buqatwjzIlnPOMZ4x5hTjzyyCPjHmEsfKKUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkxsSGMsm6JMeNew5J0r5t8bgH2JWqeue4Z5AkaWRPlEmWJbknyZVJvpXk6iQHJzkzyR1J7kqyPsmBw/Ebk6xMsl+SDUnuHo5577B/dZKtSbYk+cywdniSa4e1W5OcMKyvGa69Mcn9SVaP6r4lSfPbqF96PRa4pKpeDDwEXABsAM6qqpcw9YT7rhnnnAQcVVXHD8dcOqx/ADi5qk4AzhvWPgrcMax9CPj0tOssB14HvAz4SJL9Zw6XZFWSzUk27/mtSpIWglGH8jtVdcuwfQVwJvDtqrpvWLsMeNWMc+4Hjk5ycZLXMxVYgC3AlUnOAXYMa6cBlwNU1Q3AEUkOG/Z9vqoeraptwPeAZ88crqrWVtXKqlq5x3cqSVoQRh3KmvHx9t2eUPV94ERgI1NPjuuGXW8APgmsADYl2d37rY9O236cCX5/VpI0OUYdyucnOXXYfiuwGViW5Jhh7W3AjdNPSHIksKiqrgEuBFYkWQQ8r6q+BLwfWAosAW4Gzh7OOx3YVlUPIUnS0zTqp6p7gfOTrAe2AquBW4GrhifCTcCnZpxzFHDpEEeADwL7AVckWQoEuKiqtidZA6xPsgV4GHjHXN+QJGlhG3Uod1TVOTPWrgdOnnlgVZ0+7cMVs1zrtFnOeRB44yzra2Z8fPyTmFWSpMn9BwckSZoEI3uirKoHAJ/kJEnzik+UkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUmPxuAeQpIXmBz/4wbhHmBNLliwZ9whj4ROlJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktSY2FAmWZfkuHHPIUnaty0e9wC7UlXvHPcMkiSN7IkyybIk9yS5Msm3klyd5OAkZya5I8ldSdYnOXA4fmOSlUn2S7Ihyd3DMe8d9q9OsjXJliSfGdYOT3LtsHZrkhOG9TXDtTcmuT/J6lHdtyRpfhv1S6/HApdU1YuBh4ALgA3AWVX1EqaecN8145yTgKOq6vjhmEuH9Q8AJ1fVCcB5w9pHgTuGtQ8Bn552neXA64CXAR9Jsv/M4ZKsSrI5yeY9v1VJ0kIw6lB+p6puGbavAM4Evl1V9w1rlwGvmnHO/cDRSS5O8nqmAguwBbgyyTnAjmHtNOBygKq6ATgiyWHDvs9X1aNVtQ34HvDsmcNV1dqqWllVK/f4TiVJC8KoQ1kzPt6+2xOqvg+cCGxk6slx3bDrDcAngRXApiS7e7/10WnbjzPB789KkibHqEP5/CSnDttvBTYDy5IcM6y9Dbhx+glJjgQWVdU1wIXAiiSLgOdV1ZeA9wNLgSXAzcDZw3mnA9uq6iEkSXqaRv1UdS9wfpL1wFZgNXArcNXwRLgJ+NSMc44CLh3iCPBBYD/giiRLgQAXVdX2JGuA9Um2AA8D75jrG5IkLWyjDuWOqjpnxtr1wMkzD6yq06d9uGKWa502yzkPAm+cZX3NjI+PfxKzSpI0uf/ggCRJk2BkT5RV9QDgk5wkaV7xiVKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqpKrGPcNESuJPjKSn5YADDhj3CHPihz/84bhHmDP777//bVW1crZ9PlFKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSY2JDWWSdUmOG/cckqR92+JxD7ArVfXOcc8gSdLIniiTLEtyT5Irk3wrydVJDk5yZpI7ktyVZH2SA4fjNyZZmWS/JBuS3D0c895h/+okW5NsSfKZYe3wJNcOa7cmOWFYXzNce2OS+5OsHtV9S5Lmt1G/9HoscElVvRh4CLgA2ACcVVUvYeoJ910zzjkJOKqqjh+OuXRY/wBwclWdAJw3rH0UuGNY+xDw6WnXWQ68DngZ8JEk+88cLsmqJJuTbN7zW5UkLQSjDuV3quqWYfsK4Ezg21V137B2GfCqGefcDxyd5OIkr2cqsABbgCuTnAPsGNZOAy4HqKobgCOSHDbs+3xVPVpV24DvAc+eOVxVra2qlVW1co/vVJK0IIw6lDXj4+27PaHq+8CJwEamnhzXDbveAHwSWAFsSrK791sfnbb9OBP8/qwkaXKMOpTPT3LqsP1WYDOwLMkxw9rbgBunn5DkSGBRVV0DXAisSLIIeF5VfQl4P7AUWALcDJw9nHc6sK2qHkKSpKdp1E9V9wLnJ1kPbAVWA7cCVw1PhJuAT8045yjg0iGOAB8E9gOuSLIUCHBRVW1PsgZYn2QL8DDwjrm+IUnSwpaqma+GztEnSpYBn6uq40fyCfdQktH8xEhacA444IBxjzAnfvjDH457hDmz//7737ar70+Z2H9wQJKkSTCyl16r6gFgXjxNSpK0k0+UkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUmPxuAeQpIVmx44d4x5hTixevG8mwydKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJakxsKJOsS3LcuOeQJO3bFo97gF2pqneOewZJkkb2RJlkWZJ7klyZ5FtJrk5ycJIzk9yR5K4k65McOBy/McnKJPsl2ZDk7uGY9w77VyfZmmRLks8Ma4cnuXZYuzXJCcP6muHaG5Pcn2T1qO5bkjS/jfql12OBS6rqxcBDwAXABuCsqnoJU0+475pxzknAUVV1/HDMpcP6B4CTq+oE4Lxh7aPAHcPah4BPT7vOcuB1wMuAjyTZf+ZwSVYl2Zxk857fqiRpIRh1KL9TVbcM21cAZwLfrqr7hrXLgFfNOOd+4OgkFyd5PVOBBdgCXJnkHGDHsHYacDlAVd0AHJHksGHf56vq0araBnwPePbM4apqbVWtrKqVe3ynkqQFYdShrBkfb9/tCVXfB04ENjL15Lhu2PUG4JPACmBTkt293/rotO3HmeD3ZyVJk2PUoXx+klOH7bcCm4FlSY4Z1t4G3Dj9hCRHAouq6hrgQmBFkkXA86rqS8D7gaXAEuBm4OzhvNOBbVX1EJIkPU2jfqq6Fzg/yXpgK7AauBW4angi3AR8asY5RwGXDnEE+CCwH3BFkqVAgIuqanuSNcD6JFuAh4F3zPUNSZIWtlTNfDV0jj5Rsgz4XFUdP5JPuIeSjOYnRtKCs2jRxP4V9T3y+OOPj3uEOZPktl19f8rC/GpKkrSXjOyl16p6AJgXT5OSJO3kE6UkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUmNhQJlmX5LhxzyFJ2rctHvcAu1JV7xz3DJIkjeyJMsmyJPckuTLJt5JcneTgJGcmuSPJXUnWJzlwOH5jkpVJ9kuyIcndwzHvHfavTrI1yZYknxnWDk9y7bB2a5IThvU1w7U3Jrk/yepR3bckaX4b9UuvxwKXVNWLgYeAC4ANwFlV9RKmnnDfNeOck4Cjqur44ZhLh/UPACdX1QnAecPaR4E7hrUPAZ+edp3lwOuAlwEfSbL/zOGSrEqyOcnmPb9VSdJCMOpQfqeqbhm2rwDOBL5dVfcNa5cBr5pxzv3A0UkuTvJ6pgILsAW4Msk5wI5h7TTgcoCqugE4Islhw77PV9WjVbUN+B7w7JnDVdXaqlpZVSv3+E4lSQvCqENZMz7evtsTqr4PnAhsZOrJcd2w6w3AJ4EVwKYku3u/9dFp248zwe/PSpImx6hD+fwkpw7bbwU2A8uSHDOsvQ24cfoJSY4EFlXVNcCFwIoki4DnVdWXgPcDS4ElwM3A2cN5pwPbquohJEl6mkb9VHUvcH6S9cBWYDVwK3DV8ES4CfjUjHOOAi4d4gjwQWA/4IokS4EAF1XV9iRrgPVJtgAPA++Y6xuSJC1sqZr5augcfaJkGfC5qjp+JJ9wDyUZzU+MpAVn0aKJ/Svqe+Txxx8f9whzJsltu/r+lIX51ZQkaS8Z2UuvVfUAMC+eJiVJ2sknSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKmxeNwDSNJCc/TRR497hDnxZ3/2Z+MeYSx8opQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGhMbyiQbkrx5Dq67LMnde/u6kqSFaWJDKUnSJJiYUCZ5e5ItSe5Mcvmw/KokX0ly/86nyyRLklyf5PYkdyX5+WH9CU+KSd6XZM2wfcpw3TuB80d8a5KkeWwiQpnkp4ALgTOq6kTgl4ddzwFOA/4Z8OvD2iPAm6pqBfBq4BNJsptPcSnwnuHakiQ9aRMRSuAM4Kqq2gZQVQ8O69dW1d9X1Vbg2cNagF9LsgX4E+Coafv+kSTPBJ5ZVTcNS5c3x65KsjnJ5j27HUnSQrF43APsxqPTtnc+NZ4NPAs4paoeS/IAcBCwgyeG/6Cn+smqai2wFiBJPZ2BJUkLy6Q8Ud4AvCXJEQBJDm+OXQp8b4jkq4EXDOt/BfxYkiOSHMjUy7VU1XZge5LThuPOnpM7kCQtSBPxRFlV30zyceDGJI8DdzSHXwn8lyR3AZuBe4ZrPJbkY8DXgb/cuT74RWD98JR43VzcgyRpYUqVrzDOxpdeJT1dxxxzzLhHmBNf+MIXxj3CnDnmmGNuq6qVs+2blJdeJUmaSIZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpMbicQ8gSQvNJZdcMu4R5sT5558/7hHGwidKSZIahlKSpIahlCSpYSglSWoYSmTh/zIAAA60SURBVEmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkxkhDmeTQJO9KklF+XkmSnq45DWWSNUneN2wfAHwSuLGqapb9H0vymuZaK5NcNJfzSpI00+JRfaKq+hHw9mb/r+7m/M3A5r09lyRJnb3+RJnkw0nuS/Jl4Nhh7UVJvpDktiQ3J1k+y3kbkrx52H5pkq8kuTPJ14eXbE9P8rlh/yFJ1g/77kjy88P6uUl+e9o1Pzec94Ikf5rkyCSLhhleu7fvXZK08OzVJ8okpwD/AjhpuPbtwG3AWuC8qvrTJC8HLgHO2MU1DgB+HzirqjYlOQz4uxmHfRi4oar+1yTPBL6e5E92NVdV/XmS3wB+B/g6sLWqrpvlc68CVj2lm5YkLWh7+6XXnwH+oKoeBkjyh8BBwCuAq6Z9D8+BzTWOBb5bVZsAquqh4VrTj3kt8M93vr85fI7nd4NV1bokbwHOYyrksx2zlqmok6S660mS9g2jeI9yEbC9qmaN09MU4Beq6t4nLE490U5/OfmgafsOBv7J8OES4G/34jySpAVqb79HeRPwxiTPSHIo8HPAw8C3h6c5MuXE5hr3As9J8tLh+EOTzAz6F4H37PxrJklOHtYfAE4a3od8HvCyaef8BnAl8KvA7+7JTUqS9h17NZRVdTtT7y/eCfwRsGnYdTbwr5LcCXwT+PnmGj8CzgIuHo7/Y6Y9GQ7+NbA/sCXJN4ePAW4Bvg1sBS5i6j1SkvzPwEuB36iqK4EfJfnFPbtbSdK+IMNfadQMvkcp6em67rp/9L2CC8InPvGJcY8wZ774xS/eVlUrZ9vnP2EnSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUmPxuAeQpIXmuuuuG/cIc+K1r33tuEeYM1/84hd3uc8nSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpMbGhTLIhyZvn4LrLkty9t68rSVqYJjaUkiRNgokJZZK3J9mS5M4klw/Lr0rylST373y6TLIkyfVJbk9yV5KfH9af8KSY5H1J1gzbpwzXvRM4f8S3JkmaxyYilEl+CrgQOKOqTgR+edj1HOA04J8Bvz6sPQK8qapWAK8GPpEku/kUlwLvGa7dzbEqyeYkm5/mrUiSFpiJCCVwBnBVVW0DqKoHh/Vrq+rvq2or8OxhLcCvJdkC/Alw1LR9/0iSZwLPrKqbhqXLd3VsVa2tqpVVtXLPbkeStFAsHvcAu/HotO2dT41nA88CTqmqx5I8ABwE7OCJ4T9oJBNKkha0SXmivAF4S5IjAJIc3hy7FPjeEMlXAy8Y1v8K+LEkRyQ5kKmXa6mq7cD2JKcNx509J3cgSVqQJuKJsqq+meTjwI1JHgfuaA6/EvgvSe4CNgP3DNd4LMnHgK8Df7lzffCLwPokBVw3F/cgSVqYJiKUAFV1GXBZs3/J8N9twKm7OOYi4KJZ1m8Dpn8jz/+5R8NKkvYZk/LSqyRJE8lQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSI1U17hkmUhJ/YiQ9LcuXLx/3CHPi2muvHfcIc2b58uW3VdXK2fb5RClJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktQwlJIkNQylJEkNQylJUsNQSpLUMJSSJDUMpSRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktSY01Am+akk/3wuP4ckSXNpt6FM8pUnccyvJDl4xtrzgQ8DG5/2dHNgtlklSdqV3Yayql7xJK7zK8AT4lNVf1FVb62qh2Y7Icl+T27Eve4fzSpJ0q48mSfKHwz/PT3JxiRXJ7knyZWZshp4LvClJF8ajn1tkq8muT3JVUmWDOsPJPmNJLcDb2mO+/UkW5NsSfL/DGtvSXJ3kjuT3DSsLUty83D+7Ule8VRnlSSps/gpHn8y8FPAfwNuAV5ZVRcluQB4dVVtS3IkcCHwmqr6YZL3AxcAHxuu8d+rasVw3GdnHpfkk8CbgOVVVUmeOZz3q8Drquovp619D/inVfVIkp8Afg9Y+WRnfYr3LknaBz3VUH69qv4rQJJvAMuAL8845qeB44BbkgAcAHx12v7f381xfwM8AvyHJJ8DPjccfwuwIcl/YiqwAPsDv53kJOBx4Cef4qxPkGQVsKo7RpK0b3mqoXx02vbjuzg/wB9X1b/cxTV+uLvjkrwMOBN4M/BLwBlVdV6SlwNvAG5LcgrwHuCvgBOZehn5kac46xNU1Vpg7TBD7e54SdLCt7f+esjfAocO27cCr0xyDECSQ5L85CznzHrc8D7l0qr6f4H3MhVBkryoqr5WVb8K/DXwPGAp8N2q+nvgbcCT+Qah6bNKktR6qk+Uu7IW+EKS/1ZVr05yLvB7SQ4c9l8I3Df9hKr6610c97fAf05yEFNPnRcM+35reB8ywPXAncAlwDVJ3g58gf/xtPqkZ316tytJ2lekylcYZ+NLr5KeruXLl497hDlx7bXXjnuEObN8+fLbqmrlbPv8J+wkSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIahlKSpIahlCSpYSglSWoYSkmSGoZSkqSGoZQkqWEoJUlqGEpJkhqGUpKkhqGUJKlhKCVJahhKSZIaqapxzzCRkvw18Ocj/JRHAttG+PlGxfuafxbqvXlf888o7+0FVfWs2XYYygmRZHNVrRz3HHub9zX/LNR7877mn0m5N196lSSpYSglSWoYysmxdtwDzBHva/5ZqPfmfc0/E3FvvkcpSVLDJ0pJkhqGUpKkhqEckSTPTPLuYfv0JJ8b90x6apJ8Zdwz7C1JVif5VpIrm2N+MPx3WZK7Rzfd3Hoy9z7CWdYked+T2Z/kY0le0xy7MslFczHnXJn2tfh+kg80x52b5LdHOdt0i8f1ifdBzwTeDVwy7kH09FTVK8Y9w170buA1VfVfxz3IGMzLe6+qX93N/s3A5hGNs7fMi6+FT5Sj8+vAi5J8A/gtYEmSq5Pck+TKJAFIckqSG5PcluSLSZ4z1qkHSS5Icvfw41eGtbcn2ZLkziSXD2vPSnJNkk3Dj1cO6y9L8tUkdyT5SpJjh/Vzk3w2yReS/GmS3xzfXfamPWE9J8lNSb4x/Hz8zLhneyqSfAo4GvijJH8z/YlmuJ9l45ptb5v563bGvb93TDN9OMl9Sb4M7Px98KLh98BtSW5OsnyW8zYkefOw/dLh99GdSb6e5NDpr1QlOTzJtcPvz1uTnDCsr5nt653kkCSfH653d5KzRvDz8ISvxc4nxiRvGWa4M8lN00557tj+nKgqf4zgB7AMuHvYPh34G+CfMPU/K18FTgP2B74CPGs47ixg/QTMfgpwF3AIsAT4JvBK4D7gyOGYw4f//kfgtGH7+cC3hu3DgMXD9muAa4btc4H7gaXAQUz9s4HPG/c97+Ln4QfDf/934MPD9n7AoeOe7WncywNM/fNga4D3TVu/G1g2437/4dfufPqxi1+3J++89zHPdPDwe+L/A94HXA/8xHDMy4Ebhu1/+PoAG4A3AwcMv2deOqwfxtSrg6cDnxvWLgY+MmyfAXxj5vWmf72BXwB+d9r60hH/OjwX+O1h7S7gqGH7mcN/x/rnhC+9js/Xa3i5YXjKXAZsB44H/nh4wNwP+O64BpzmNOAPquqHAEk+C6wErqqqbQBV9eBw7GuA44b5AQ5LsoSpX+CXJfkJoJj6n4Kdrq+qvxmuvRV4AfCdub2lPbIJWJ9kf+DaqvrGuAfSrGb7dTvup/+fYWqmhwGS/CFTf/C/Arhq2u+bA5trHAt8t6o2AVTVQ8O1ph9zGlPxo6puSHJEksOaa94FfCLJbzAV25uf6o3tRbcAG5L8J+Cz09bH9ueEoRyfR6dtP87U1yLAN6vq1PGMtFcsAn66qh6Zvji8rPKlqnrT8NLexmm7Z/u5mFhVdVOSVwFvYOo39L+pqk+Pe66naQdPfAvmoHENsg9bBGyvqpNG8Llm/XpX1X1JVgD/C/B/Jbm+qj42gnn+kao6L8nLmfr9dVuSU4ZdY/tzwvcoR+dvgUN3c8y9wLOSnAqQZP8kPzXnk+3ezcAbkxyc5BDgTUx908BbkhwBU++JDMdeB7xn54lJdv7mXwr85bB97iiGnitJXgD8VVX9LrAOWDHmkfbEAwzzD39QvnCs0+xds/26HeeTEsBNw0zPSHIo8HPAw8C3k7wFIFNObK5xL/CcJC8djj80ycxo3AycPew/Hdg2PHk+wCxf7yTPBR6uqiuY+h6Ksf2aTvKiqvpaTX3z0l8DzxvXLDtN9P+5LyRV9d+T3JKpb7P/O+CvZjnmR8Ob9RclWcrU1+ffMfXeythU1e1JNgBfH5bWVdUtST4O3JjkceAOpgK4Gvhkki1MzX8TcB7wm0y99Hoh8PkR38LedjrwfyR5DPgB8PbxjrNHrgHenuSbwNeYet95QdjFr9s7ZrxEOY6Zfh+4E/geUy/jw1TUfmf4/bE/8JnhmNmu8aPhm20uTvIMpv48mfnXRtYw9fbAFqZC/I5hfVdf75cAv5Xk74HHgHft6b3ugd8a3qIJU+/d3gmM4ml7l/wn7CRJavjSqyRJDUMpSVLDUEqS1DCUkiQ1DKUkSQ1DKUlSw1BKktT4/wGS3SsmJbC32AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URuGJcK8mWnr"
      },
      "source": [
        " "
      ],
      "execution_count": 101,
      "outputs": []
    }
  ]
}